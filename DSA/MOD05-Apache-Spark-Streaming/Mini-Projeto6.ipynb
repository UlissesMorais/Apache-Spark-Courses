{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b9e32795-efee-4635-ab3b-96a69ac3e973",
   "metadata": {},
   "source": [
    "# <font color='blue'>Data Science Academy</font>\n",
    "# <font color='blue'>Big Data Real-Time Analytics com Python e Spark</font>\n",
    "\n",
    "## <font color='blue'>Mini-Projeto 6</font>\n",
    "\n",
    "### <font color='blue'>Análise de Dados de Sensores IoT em Tempo Real com Apache Spark Streaming e Apache Kafka</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da5ccdd6",
   "metadata": {},
   "source": [
    "![title](imagens/MP6.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dbafefd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Versão da Linguagem Python Usada Neste Jupyter Notebook: 3.9.12\n"
     ]
    }
   ],
   "source": [
    "# Versão da Linguagem Python\n",
    "from platform import python_version\n",
    "print('Versão da Linguagem Python Usada Neste Jupyter Notebook:', python_version())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "de77becb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Para atualizar um pacote, execute o comando abaixo no terminal ou prompt de comando:\n",
    "# pip install -U nome_pacote\n",
    "\n",
    "# Para instalar a versão exata de um pacote, execute o comando abaixo no terminal ou prompt de comando:\n",
    "#!pip install nome_pacote==versão_desejada\n",
    "\n",
    "# Depois de instalar ou atualizar o pacote, reinicie o jupyter notebook.\n",
    "\n",
    "# Instala o pacote watermark. \n",
    "# Esse pacote é usado para gravar as versões de outros pacotes usados neste jupyter notebook.\n",
    "#!pip install -q -U watermark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f50a5691",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install -q findspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3183aeb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importa o findspark e inicializa\n",
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e14c6950-525d-4e54-a25e-13e9d6818c67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required modules\n",
    "import pyspark\n",
    "from pyspark.streaming import StreamingContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, DoubleType\n",
    "from pyspark.sql.functions import col, from_json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d29efd4a",
   "metadata": {},
   "source": [
    "> Precisamos incluir o conector de integração do Spark Streaming com o Apache Kafka. Fique atento à versão do PySpark que está sendo usada.\n",
    "\n",
    "https://spark.apache.org/docs/latest/structured-streaming-kafka-integration.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5172de34-9787-4f22-b1a7-8e9bb3b7cbf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conector\n",
    "import os\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.3.0 pyspark-shell'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "488dcaf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Author: Data Science Academy\n",
      "\n",
      "findspark: 2.0.1\n",
      "pyspark  : 3.3.1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Versões dos pacotes usados neste jupyter notebook\n",
    "%reload_ext watermark\n",
    "%watermark -a \"Data Science Academy\" --iversions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6efeedac",
   "metadata": {},
   "source": [
    "## Criando a Sessão Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a35f1973-363c-4112-8440-62fca2410984",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cria a sessão Spark\n",
    "spark = SparkSession.builder.appName(\"Mini-Projeto6\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad8744ef",
   "metadata": {},
   "source": [
    "## Leitura do Kafka Spark Structured Stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fa0de89c-e700-4754-9175-d95777b95a18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vamos criar uma subscrição no tópico que tem o streaming de dados que desejamos \"puxar\" os dados.\n",
    "df = spark \\\n",
    "  .readStream \\\n",
    "  .format(\"kafka\") \\\n",
    "  .option(\"kafka.bootstrap.servers\", \"localhost:9092\") \\\n",
    "  .option(\"subscribe\", \"dsamp6\") \\\n",
    "  .load()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc60991a",
   "metadata": {},
   "source": [
    "## Definição do Schema da Fonte de Dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c92c2301-7f4c-48fd-8e5a-cc12384ad834",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definimos o schema dos dados que desejamos capturar para análise (temperatura)\n",
    "esquema_dados_temp = StructType([StructField(\"leitura\", \n",
    "                                             StructType([StructField(\"temperatura\", DoubleType(), True)]), True)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6f0a8f63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definimos o schema global dos dados no streaming\n",
    "esquema_dados = StructType([ \n",
    "    StructField(\"id_sensor\", StringType(), True), \n",
    "    StructField(\"id_equipamento\", StringType(), True), \n",
    "    StructField(\"sensor\", StringType(), True), \n",
    "    StructField(\"data_evento\", StringType(), True), \n",
    "    StructField(\"padrao\", esquema_dados_temp, True)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf910ee0",
   "metadata": {},
   "source": [
    "## Parse da Fonte de Dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "78a505e1-6004-48ab-8275-0cdb96de583a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Capturamos cada linha de dado (cada valor) como string\n",
    "df_conversao = df.selectExpr(\"CAST(value AS STRING)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7bcc58eb-a9b5-4760-8c09-abe6bdd117b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse do formato JSON em dataframe\n",
    "df_conversao = df_conversao.withColumn(\"jsonData\", from_json(col(\"value\"), esquema_dados)).select(\"jsonData.*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "77250a76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id_sensor: string (nullable = true)\n",
      " |-- id_equipamento: string (nullable = true)\n",
      " |-- sensor: string (nullable = true)\n",
      " |-- data_evento: string (nullable = true)\n",
      " |-- padrao: struct (nullable = true)\n",
      " |    |-- leitura: struct (nullable = true)\n",
      " |    |    |-- temperatura: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_conversao.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70d91c83",
   "metadata": {},
   "source": [
    "## Preparamos o Dataframe \n",
    "\n",
    "Esse dataframe está no formato que precisamos para análise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f9734cc6-fd90-4f32-aa7f-dafcf5bc5688",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Renomeamos as colunas para simplificar nossa análise\n",
    "df_conversao_temp_sensor = df_conversao.select(col(\"padrao.leitura.temperatura\").alias(\"temperatura\"), \n",
    "                                               col(\"sensor\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2d725e5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- temperatura: double (nullable = true)\n",
      " |-- sensor: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_conversao_temp_sensor.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "39a99b90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Não podemos visualizar o dataframe, pois a fonte é de streaming\n",
    "# df_conversao_temp_sensor.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bef4ea01",
   "metadata": {},
   "source": [
    "## Análise de Dados em Tempo Real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "16da9087-67a4-4f16-8325-7d25dcd8d5aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aqui temos o objeto que irá conter nossa análise, o cálculo da média das temperaturas por sensor\n",
    "df_media_temp_sensor = df_conversao_temp_sensor.groupby(\"sensor\").mean(\"temperatura\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4c515750",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- sensor: string (nullable = true)\n",
      " |-- avg(temperatura): double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_media_temp_sensor.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c666eb59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Renomeamos as colunas para simplificar nossa análise\n",
    "df_media_temp_sensor = df_media_temp_sensor.select(col(\"sensor\").alias(\"sensor\"), \n",
    "                                                   col(\"avg(temperatura)\").alias(\"media_temp\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "59dedc8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- sensor: string (nullable = true)\n",
      " |-- media_temp: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_media_temp_sensor.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60fe8621",
   "metadata": {},
   "source": [
    "Abaixo abrimos o streaming para análise de dados em tempo real, imprimindo o resultado no console."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b3946d80-165e-47a5-a921-56e3d6e675d2",
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o70.start.\n: java.io.IOException: Cannot run program \"C:\\hadoop\\bin\\winutils.exe\": CreateProcess error=216, Esta versão de %1 não é compatível com a versão do Windows sendo executada. Verifique as informações de sistema do computador e contate o fornecedor do software\r\n\tat java.base/java.lang.ProcessBuilder.start(ProcessBuilder.java:1128)\r\n\tat java.base/java.lang.ProcessBuilder.start(ProcessBuilder.java:1071)\r\n\tat org.apache.hadoop.util.Shell.runCommand(Shell.java:934)\r\n\tat org.apache.hadoop.util.Shell.run(Shell.java:900)\r\n\tat org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:1212)\r\n\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:1306)\r\n\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:1288)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSystem.java:978)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:324)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:294)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.createOutputStreamWithMode(RawLocalFileSystem.java:439)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459)\r\n\tat org.apache.hadoop.fs.FileSystem.primitiveCreate(FileSystem.java:1305)\r\n\tat org.apache.hadoop.fs.DelegateToFileSystem.createInternal(DelegateToFileSystem.java:102)\r\n\tat org.apache.hadoop.fs.ChecksumFs$ChecksumFSOutputSummer.<init>(ChecksumFs.java:353)\r\n\tat org.apache.hadoop.fs.ChecksumFs.createInternal(ChecksumFs.java:400)\r\n\tat org.apache.hadoop.fs.AbstractFileSystem.create(AbstractFileSystem.java:626)\r\n\tat org.apache.hadoop.fs.FileContext$3.next(FileContext.java:701)\r\n\tat org.apache.hadoop.fs.FileContext$3.next(FileContext.java:697)\r\n\tat org.apache.hadoop.fs.FSLinkResolver.resolve(FSLinkResolver.java:90)\r\n\tat org.apache.hadoop.fs.FileContext.create(FileContext.java:703)\r\n\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.createTempFile(CheckpointFileManager.scala:327)\r\n\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.<init>(CheckpointFileManager.scala:140)\r\n\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.<init>(CheckpointFileManager.scala:143)\r\n\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.createAtomic(CheckpointFileManager.scala:333)\r\n\tat org.apache.spark.sql.execution.streaming.StreamMetadata$.write(StreamMetadata.scala:79)\r\n\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$streamMetadata$1(StreamExecution.scala:140)\r\n\tat scala.Option.getOrElse(Option.scala:189)\r\n\tat org.apache.spark.sql.execution.streaming.StreamExecution.<init>(StreamExecution.scala:138)\r\n\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.<init>(MicroBatchExecution.scala:49)\r\n\tat org.apache.spark.sql.streaming.StreamingQueryManager.createQuery(StreamingQueryManager.scala:279)\r\n\tat org.apache.spark.sql.streaming.StreamingQueryManager.startQuery(StreamingQueryManager.scala:326)\r\n\tat org.apache.spark.sql.streaming.DataStreamWriter.startQuery(DataStreamWriter.scala:427)\r\n\tat org.apache.spark.sql.streaming.DataStreamWriter.startInternal(DataStreamWriter.scala:406)\r\n\tat org.apache.spark.sql.streaming.DataStreamWriter.start(DataStreamWriter.scala:249)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:834)\r\nCaused by: java.io.IOException: CreateProcess error=216, Esta versão de %1 não é compatível com a versão do Windows sendo executada. Verifique as informações de sistema do computador e contate o fornecedor do software\r\n\tat java.base/java.lang.ProcessImpl.create(Native Method)\r\n\tat java.base/java.lang.ProcessImpl.<init>(ProcessImpl.java:487)\r\n\tat java.base/java.lang.ProcessImpl.start(ProcessImpl.java:154)\r\n\tat java.base/java.lang.ProcessBuilder.start(ProcessBuilder.java:1107)\r\n\t... 47 more\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Input \u001b[1;32mIn [22]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Objeto que inicia a consulta ao streaming com formato de console\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m query \u001b[38;5;241m=\u001b[39m \u001b[43mdf_media_temp_sensor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwriteStream\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutputMode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcomplete\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mconsole\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pyspark\\sql\\streaming.py:1389\u001b[0m, in \u001b[0;36mDataStreamWriter.start\u001b[1;34m(self, path, format, outputMode, partitionBy, queryName, **options)\u001b[0m\n\u001b[0;32m   1387\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mqueryName(queryName)\n\u001b[0;32m   1388\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1389\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sq(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m   1390\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1391\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sq(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jwrite\u001b[38;5;241m.\u001b[39mstart(path))\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\py4j\\java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1315\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1316\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1320\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1321\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1322\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1324\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[0;32m   1325\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pyspark\\sql\\utils.py:190\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    188\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    189\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 190\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39ma, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n\u001b[0;32m    191\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    192\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\py4j\\protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[1;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[0;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[0;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[0;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o70.start.\n: java.io.IOException: Cannot run program \"C:\\hadoop\\bin\\winutils.exe\": CreateProcess error=216, Esta versão de %1 não é compatível com a versão do Windows sendo executada. Verifique as informações de sistema do computador e contate o fornecedor do software\r\n\tat java.base/java.lang.ProcessBuilder.start(ProcessBuilder.java:1128)\r\n\tat java.base/java.lang.ProcessBuilder.start(ProcessBuilder.java:1071)\r\n\tat org.apache.hadoop.util.Shell.runCommand(Shell.java:934)\r\n\tat org.apache.hadoop.util.Shell.run(Shell.java:900)\r\n\tat org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:1212)\r\n\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:1306)\r\n\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:1288)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSystem.java:978)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:324)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:294)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.createOutputStreamWithMode(RawLocalFileSystem.java:439)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:428)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:459)\r\n\tat org.apache.hadoop.fs.FileSystem.primitiveCreate(FileSystem.java:1305)\r\n\tat org.apache.hadoop.fs.DelegateToFileSystem.createInternal(DelegateToFileSystem.java:102)\r\n\tat org.apache.hadoop.fs.ChecksumFs$ChecksumFSOutputSummer.<init>(ChecksumFs.java:353)\r\n\tat org.apache.hadoop.fs.ChecksumFs.createInternal(ChecksumFs.java:400)\r\n\tat org.apache.hadoop.fs.AbstractFileSystem.create(AbstractFileSystem.java:626)\r\n\tat org.apache.hadoop.fs.FileContext$3.next(FileContext.java:701)\r\n\tat org.apache.hadoop.fs.FileContext$3.next(FileContext.java:697)\r\n\tat org.apache.hadoop.fs.FSLinkResolver.resolve(FSLinkResolver.java:90)\r\n\tat org.apache.hadoop.fs.FileContext.create(FileContext.java:703)\r\n\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.createTempFile(CheckpointFileManager.scala:327)\r\n\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.<init>(CheckpointFileManager.scala:140)\r\n\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.<init>(CheckpointFileManager.scala:143)\r\n\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.createAtomic(CheckpointFileManager.scala:333)\r\n\tat org.apache.spark.sql.execution.streaming.StreamMetadata$.write(StreamMetadata.scala:79)\r\n\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$streamMetadata$1(StreamExecution.scala:140)\r\n\tat scala.Option.getOrElse(Option.scala:189)\r\n\tat org.apache.spark.sql.execution.streaming.StreamExecution.<init>(StreamExecution.scala:138)\r\n\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.<init>(MicroBatchExecution.scala:49)\r\n\tat org.apache.spark.sql.streaming.StreamingQueryManager.createQuery(StreamingQueryManager.scala:279)\r\n\tat org.apache.spark.sql.streaming.StreamingQueryManager.startQuery(StreamingQueryManager.scala:326)\r\n\tat org.apache.spark.sql.streaming.DataStreamWriter.startQuery(DataStreamWriter.scala:427)\r\n\tat org.apache.spark.sql.streaming.DataStreamWriter.startInternal(DataStreamWriter.scala:406)\r\n\tat org.apache.spark.sql.streaming.DataStreamWriter.start(DataStreamWriter.scala:249)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:834)\r\nCaused by: java.io.IOException: CreateProcess error=216, Esta versão de %1 não é compatível com a versão do Windows sendo executada. Verifique as informações de sistema do computador e contate o fornecedor do software\r\n\tat java.base/java.lang.ProcessImpl.create(Native Method)\r\n\tat java.base/java.lang.ProcessImpl.<init>(ProcessImpl.java:487)\r\n\tat java.base/java.lang.ProcessImpl.start(ProcessImpl.java:154)\r\n\tat java.base/java.lang.ProcessBuilder.start(ProcessBuilder.java:1107)\r\n\t... 47 more\r\n"
     ]
    }
   ],
   "source": [
    "# Objeto que inicia a consulta ao streaming com formato de console\n",
    "query = df_media_temp_sensor.writeStream.outputMode(\"complete\").format(\"console\").start()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61e8a6d2",
   "metadata": {},
   "source": [
    "Envie novos arquivos para o Kafka a fim de ver a análise em tempo real por aqui. Clique no botão Stop no menu superior para interromper a célula a qualquer momento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5792c3b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Executamos a query do streaming e evitamos que o processo seja encerrado\n",
    "query.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41028bf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "query.status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ae4ad89",
   "metadata": {},
   "outputs": [],
   "source": [
    "query.lastProgress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b4e69b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "query.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e469f93",
   "metadata": {},
   "source": [
    "## Análise de Dados em Tempo Real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65867a47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Objeto que inicia a consulta ao streaming com formato de memória (cria tabela temporária)\n",
    "query_memoria = df_media_temp_sensor \\\n",
    "    .writeStream \\\n",
    "    .queryName(\"dsa\") \\\n",
    "    .outputMode(\"complete\") \\\n",
    "    .format(\"memory\") \\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "706168fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Streams ativados\n",
    "spark.streams.active"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac82b405",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vamos manter a query executando por algum tempo e aplicando SQL aos dados em tempo real\n",
    "from time import sleep\n",
    "\n",
    "for x in range(10):\n",
    "    \n",
    "    spark.sql(\"select sensor, round(media_temp, 2) as media from dsa where media_temp > 65\").show()\n",
    "    sleep(3)\n",
    "    \n",
    "query_memoria.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36441eef",
   "metadata": {},
   "source": [
    "# Fim"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
